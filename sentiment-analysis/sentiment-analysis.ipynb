{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:51:28.714002Z",
     "start_time": "2024-11-11T16:51:28.522001Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:51:31.048296Z",
     "start_time": "2024-11-11T16:51:28.731701Z"
    }
   },
   "cell_type": "code",
   "source": "data = pd.read_csv('Sentiment140-dataset.csv', encoding =\"ISO-8859-1\")",
   "id": "82dcaf8091c80152",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:51:31.087536Z",
     "start_time": "2024-11-11T16:51:31.085222Z"
    }
   },
   "cell_type": "code",
   "source": "data.rename(columns={\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\": 'tweets'}, inplace=True)",
   "id": "ef683266f49c771f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:51:31.304171Z",
     "start_time": "2024-11-11T16:51:31.138404Z"
    }
   },
   "cell_type": "code",
   "source": "print(np.sum(data.isnull().any(axis=1)))  # checking null values",
   "id": "2bb5d62be6361002",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:51:31.315586Z",
     "start_time": "2024-11-11T16:51:31.313084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = data.iloc[:, -1]\n",
    "y = data.iloc[:, 0]"
   ],
   "id": "a24791eaa1ff240c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:51:34.978748Z",
     "start_time": "2024-11-11T16:51:31.372181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = X.replace(r'http\\S+|https\\S+', '', regex=True) # remove URLs\n",
    "X = X.replace(r'@\\w+', '', regex=True) # remove mentions (@user)\n",
    "X = X.replace(r'#', '', regex=True) # remove hashtags\n",
    "X = X.replace(r'[^a-zA-Z\\s]', '', regex=True) # remove special characters and numbers (keeping only letters and whitespace)"
   ],
   "id": "2890ad57cffb1c76",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:51:35.354883Z",
     "start_time": "2024-11-11T16:51:34.985883Z"
    }
   },
   "cell_type": "code",
   "source": "X = X.str.lower() # convert text to lowercase",
   "id": "3cd6e596ebdac416",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:52:43.892212Z",
     "start_time": "2024-11-11T16:51:35.361077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "X_tokenized = X.apply(word_tokenize) # tokenize the text"
   ],
   "id": "5bcf0c365d8a4a02",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T17:01:44.908433Z",
     "start_time": "2024-11-11T17:01:39.152039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "X_filtered = X_tokenized.apply(lambda tokens: [word for word in tokens if word not in stop_words])"
   ],
   "id": "989ff375f20516b1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T17:04:25.171429Z",
     "start_time": "2024-11-11T17:02:44.232352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer() # initialize the stemmer\n",
    "X_stemmed = X_filtered.apply(lambda x: [stemmer.stem(word) for word in x]) # apply stemming"
   ],
   "id": "3602bc2a67918e1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T17:20:20.513152Z",
     "start_time": "2024-11-11T17:20:19.101985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk \n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer() # initialize the lemmatizer"
   ],
   "id": "acfbc7d1d41a7718",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/slavka/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/slavka/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/slavka/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T17:22:46.653457Z",
     "start_time": "2024-11-11T17:22:46.650710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to map the part of speech (POS) to the format that WordNetLemmatizer accepts\n",
    "def get_wordnet_pos(word):\n",
    "    # return the POS tag of the word in tupple ('running', 'VGB'), \n",
    "    # [0][1][0] extract the first letter of the POS tag 'VGB', so it becomes 'V'\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper() \n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV} # mapping\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ],
   "id": "2d33e5b312193cbe",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:21:56.568175Z",
     "start_time": "2024-11-11T17:23:26.439973Z"
    }
   },
   "cell_type": "code",
   "source": "X_lemmatized = X.apply(lambda tokens: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens])",
   "id": "3db72addebad5e62",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
